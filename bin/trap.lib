# Copyright 2016-2019 Jean-Laurent Huynh
#
# Licensed under the Apache License, Version 2.0 (the "License"): you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

# The file extension is lib to avoid using .py so that Splunk does not consider
# this file as a modular input. Instead the binary specific trap.sh should
# be called.

import sys
import xml.dom.minidom, xml.sax.saxutils
import logging
import time
import json
import os
import csv
import re
import dateparser
import html2text
from datetime import datetime, timedelta
from exchangelib import DELEGATE, IMPERSONATION, Account, Credentials, ServiceAccount, \
    EWSDateTime, EWSTimeZone, Configuration, NTLM, GSSAPI, CalendarItem, Message, Mailbox, \
    Attendee, Q, ExtendedProperty, FileAttachment, ItemAttachment, HTMLBody, Build, Version, \
    FolderCollection, UTC, Folder
from exchangelib.errors import ErrorFolderNotFound


#set up logging suitable for splunkd comsumption
logging.root
# Do not use DEBUG on a permanent basis, exchangelib is quite verbose
logging.root.setLevel(logging.WARNING)
formatter = logging.Formatter('%(levelname)s %(message)s')
handler = logging.StreamHandler()
handler.setFormatter(formatter)
logging.root.addHandler(handler)

MASK = "--------"

SCHEME = """<scheme>
    <title>TRAP</title>
    <description>Process the Proofpoint TRAP inbox</description>
    <use_external_validation>true</use_external_validation>
    <streaming_mode>simple</streaming_mode>
    <endpoint>
        <args>
            <arg name="name">
                <title>Input Name</title>
                <description>Name parameter description.</description>
            </arg>

            <arg name="mailbox">
                <title>Mailbox</title>
                <description>mailbox to monitor (e.g. proofpointdetection.serviceaccount@example.com)</description>
            </arg>

            <arg name="domain">
                <title>Domain</title>
                <description>domain name</description>
            </arg>

            <arg name="username">
                <title>Username</title>
                <description>username to access mailbox</description>
            </arg>

            <arg name="password">
                <title>Password</title>
                <description>password for username</description>
            </arg>

            <arg name="folder">
                <title>Folder</title>
                <description>Folder to monitor (falls back to Inbox if missing)</description>
                <required_on_create>false</required_on_create>
            </arg>

            <arg name="max_count">
                <title>Maximum Count</title>
                <description>Cap number of emails retrieve per invocation, defaults to 100 if not provided</description>
            </arg>

            <arg name="lag_seconds">
                <title>Processing Delay</title>
                <description>Minimum number of seconds from time reported before message is considered for processing, defaults to 3600</description>
            </arg>

        </args>
    </endpoint>
</scheme>
"""

def encrypt_password(username, password, session_key):
    import splunklib.client as client
    args = {'token':session_key}
    service = client.connect(**args)
    
    try:
        # If the credential already exists, delete it.
        for storage_password in service.storage_passwords:
            if storage_password.username == username:
                service.storage_passwords.delete(username=storage_password.username)
                break

        # Create the credential.
        service.storage_passwords.create(password, username)

    except Exception as e:
        raise Exception, "An error occurred updating credentials. Please ensure your user account has admin_all_objects and/or list_storage_passwords capabilities. Details: %s" % str(e)

def mask_password(full_input_name, session_key, username):
    import splunklib.client as client
    try:
        args = {'token':session_key}
        service = client.connect(**args)
        kind, input_name = full_input_name.split("://")
        item = service.inputs.__getitem__((input_name, kind))
        
        kwargs = {
            "username": username,
            "password": MASK
        }
        item.update(**kwargs).refresh()
            
    except Exception as e:
        raise Exception("Error updating inputs.conf: %s" % str(e))

def get_password(session_key, username):
    import splunklib.client as client
    args = {'token':session_key}
    service = client.connect(**args)

    # Retrieve the password from the storage/passwords endpoint	
    for storage_password in service.storage_passwords:
        if storage_password.username == username:
            return storage_password.content.clear_password

def do_scheme():
	print SCHEME

# prints XML error data to be consumed by Splunk
def print_error(s):
    print "<error><message>%s</message></error>" % xml.sax.saxutils.escape(s)

def validate_conf(config, key):
    if key not in config:
        raise Exception, "Invalid configuration received from Splunk: key '%s' is missing." % key

#read XML configuration passed from splunkd
def get_config():
    config = {}

    try:
        # read everything from stdin
        config_str = sys.stdin.read()

        # parse the config XML
        doc = xml.dom.minidom.parseString(config_str)
        root = doc.documentElement
        conf_node = root.getElementsByTagName("configuration")[0]
        if conf_node:
            logging.debug("XML: found configuration")
            stanza = conf_node.getElementsByTagName("stanza")[0]
            if stanza:
                stanza_name = stanza.getAttribute("name")
                if stanza_name:
                    logging.debug("XML: found stanza " + stanza_name)
                    config["name"] = stanza_name

                    params = stanza.getElementsByTagName("param")
                    for param in params:
                        param_name = param.getAttribute("name")
                        logging.debug("XML: found param '%s'" % param_name)
                        if param_name and param.firstChild and \
                           param.firstChild.nodeType == param.firstChild.TEXT_NODE:
                            data = param.firstChild.data
                            config[param_name] = data
                            #logging.debug("XML: '%s' -> '%s'" % (param_name, data))

        checkpnt_node = root.getElementsByTagName("checkpoint_dir")[0]
        if checkpnt_node and checkpnt_node.firstChild and \
           checkpnt_node.firstChild.nodeType == checkpnt_node.firstChild.TEXT_NODE:
            config["checkpoint_dir"] = checkpnt_node.firstChild.data
        checkpnt_node = root.getElementsByTagName("session_key")[0]
        if checkpnt_node and checkpnt_node.firstChild and \
           checkpnt_node.firstChild.nodeType == checkpnt_node.firstChild.TEXT_NODE:
            config["session_key"] = checkpnt_node.firstChild.data

        if not config:
            raise Exception, "Invalid configuration received from Splunk."

        # just some validation: make sure these keys are present (required)
        validate_conf(config, "name")
        validate_conf(config, "mailbox")
        validate_conf(config, "domain")
        validate_conf(config, "username")
        validate_conf(config, "password")
        validate_conf(config, "checkpoint_dir")
        validate_conf(config, "session_key")
    except Exception, e:
        raise Exception, "Error getting Splunk configuration via STDIN: %s" % str(e)

    return config

def get_validation_data():
    val_data = {}

    # read everything from stdin
    val_str = sys.stdin.read()

    # parse the validation XML
    doc = xml.dom.minidom.parseString(val_str)
    root = doc.documentElement

    logging.debug("XML: found items")
    item_node = root.getElementsByTagName("item")[0]
    if item_node:
        logging.debug("XML: found item")

        name = item_node.getAttribute("name")
        val_data["stanza"] = name

        params_node = item_node.getElementsByTagName("param")
        for param in params_node:
            name = param.getAttribute("name")
            logging.debug("Found param %s" % name)
            if name and param.firstChild and \
               param.firstChild.nodeType == param.firstChild.TEXT_NODE:
                val_data[name] = param.firstChild.data

    return val_data

def validate_config(username, password):
    try:
        pass
    except Exception,e:
        print_error("Invalid configuration specified: %s" % str(e))
        sys.exit(1)	

def load_checkpoint(config):
    """
    Load the most recent epoch and message_ids for this second processed.
    Return an (epoch, [message_id]) tuple to start ingesting from or None.
    Config is the config provided to the script.
    """
    filename = os.path.join(config["checkpoint_dir"], 'epoch')
    if os.path.exists(filename):
        with open(filename, 'r') as f:
            content = f.read().splitlines()
            epoch = int(content[0])
            message_ids = content[1:]
            return (epoch, message_ids)
    else:
        return None

def save_checkpoint(config, epoch, message_ids):
    """
    Save the checkpoint (the last epoch processed as well as all message ids for that epoch).
    """
    filename = os.path.join(config["checkpoint_dir"], 'epoch')
    with open(filename, 'w') as f:
        f.write(str(int(epoch)) + '\n')
        for message_id in message_ids:
            f.write(message_id + '\n')


# Put all EWS related function into one class to control import
class Poller:

    tz = EWSTimeZone.timezone('America/Los_Angeles')
    tz_local = EWSTimeZone.localzone()

    def check_attachment(self, attachment):
        """Returns the date and time this email attachment was received in the local timezone"""
        if not attachment.item.datetime_received:
            return None
        return attachment.item.datetime_received.astimezone(self.tz)


    def try_date_format(self, str_date, str_format):
        """Tries to parse str_date using str_format, returns None if unsuccessful"""
        try:
            return datetime.strptime(str_date, str_format)
        except ValueError:
            return None


    def try_date_formats(self, str_date, formats):
        """Tries to parse str_date using each format string in formats, falls back to dateparser.parse.
        Returns None if unsuccessful"""
        for f in formats:
            d = self.try_date_format(str_date, f)
            if d:
                return d
        d = dateparser.parse(str_date)
        return d


    def extract(self, item):
        """
        Extracts key information from the message and its attachments

        All date time are returned in local timezones.
        """

        ews_id = item.id
        user = item.sender.email_address
        display_from = None
        subject = item.subject
        pulled = None
        received = None
        message_id = item.message_id
        conversation_id = item.conversation_id.id
        in_reply_to = item.in_reply_to
        text_body = None

        pulled = item.datetime_received.astimezone(self.tz)

        text_body = item.text_body

        if text_body:
            sent_match = re.search(r'(?ms)Sent: ([^\n\r]*)', text_body)
            if not sent_match:
                sent_match = re.search(r'(?ms)Date: ([^\n\r]*)', text_body)
            if sent_match:
                # the Sent line is populated by trap according to this format
                received = self.try_date_formats(sent_match.group(1),
                        [   '%A, %B %d, %Y %I:%M:%S %p' # Tuesday, January 29, 2019 8:53:53 PM 
                            ])
                if not received:
                    logging.warning('TODO: handle ' + sent_match.group(1))
                else:
                    # we are making a guess here that the user timezone is Pacific
                    received = self.tz.localize(received)
            else:
                for attachment in item.attachments:
                    if isinstance(attachment, ItemAttachment) and \
                            isinstance(attachment.item, Message):
                        received = self.check_attachment(attachment)
                        break
            from_match = re.search(r'(?ms)From: ([^\n\r]*)', text_body)
            if from_match:
                display_from = from_match.group(1)
        else:
            logging.warning('TODO: no item.text_body')

        return {
            'ews_id': ews_id,
            'user': user.lower(),
            'display_from': display_from,
            'subject': subject,
            'received': received,
            'pulled': pulled,
            'message_id': message_id,
            'conversation_id': conversation_id,
            'in_reply_to': in_reply_to
            #, 'text_body': text_body
        }

    def extract_or_log(self, item):
        try:
            return self.extract(item)
        except Exception as e:
            logging.exception("Unable to extract email: %s" % str(e))

    def get_body(self, row):
        """Read the template from disk from disposition and return the body to send back."""
        if row['disposition']:
            filename = re.sub(r'[^0-9A-Za-z_]', '_', row['disposition']) + '.txt'
            with open(filename, 'r') as file:
                try:
                    text = file.read()
                    return text
                except IOError as e:
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    logging.error("Unable to get body for auto-reply: %s at line %d" % (str(e), exc_tb.tb_lineno))
                    return None
        else:
            return None

    def poll(self, domain, user, mailbox, password, folder_name, max_count, lag_seconds, from_epoch_and_ids):
        """Poll the mailbox"""

        credentials = Credentials(domain + '\\' + user, password)
        account = Account(mailbox, credentials=credentials, autodiscover=True)

        if folder_name:
            fc = account.msg_folder_root.glob('**/' + folder_name)
            if len(fc.folders) == 1:
                folder = fc.folders[0]
            else:
                logging.warning('Cannot find folder or ambiguous folder in', account.msg_folder_root.tree())
                return
        else:
            folder = account.inbox

        logging.info('Searching in folder ' + folder.name)

        epoch = datetime.utcfromtimestamp(0).replace(tzinfo=UTC)
        def unix_time_epoch(row, k):
            if k in row and row[k]:
                return (row[k].astimezone(UTC) - epoch).total_seconds()
            return None

        # leave the items in the inbox for some time before processing
        cutoff = datetime.utcnow().replace(tzinfo=UTC).astimezone(self.tz) - timedelta(seconds=lag_seconds)
        # for the start date time, we use the from_epoch, but for the same second we need to skip message ids
        # in from_epoch_and_ids[1]
        if from_epoch_and_ids:
            from_dt = datetime.utcfromtimestamp(from_epoch_and_ids[0]).replace(tzinfo=UTC).astimezone(self.tz)
            logging.warning('from_epoch ' + str(from_epoch_and_ids[0]) + ' ' + str(from_dt) + ' ' + ';'.join(from_epoch_and_ids[1]))
            # paginate in chunks of max_count
            page_start = 0
            while True:
                #logging.warning('paginate from [%d to %d) ' % (page_start, page_start+max_count))
                # items0 may contain messages already processed
                items0 = folder.all().filter(
                        datetime_received__range=(from_dt, cutoff)
                    ).order_by(
                            'datetime_received', 'message_id'
                    )[(page_start):(page_start+max_count)]
                # filter out items that have same epoch as from_epoch_and_ids[0] and message_idin from_epoch_and_ids[1]
                items0 = list(items0)
                if len(items0) == 0:
                    break
                items = []
                for item in items0:
                    item_epoch = (item.datetime_received.astimezone(UTC) - epoch).total_seconds()
                    if (item_epoch > from_epoch_and_ids[0]) or (item.message_id not in from_epoch_and_ids[1]):
                        items.append(item)

                # if we have any items left, then we return that
                if len(items) > 0:
                    break
                page_start += max_count
        else:
            items = folder.all().filter(datetime_received__lt=cutoff).order_by('datetime_received', 'message_id')[0:max_count]

        rows = [self.extract_or_log(item) for item in items ]

        def default(o):
            if isinstance(o, (datetime, EWSDateTime)):
                return o.isoformat()
            # Let the base class default method raise the TypeError
            raise TypeError('cannot serialize ' + str(o))

        total_count = 0
        processed_count = 0

        # Prepare next checkpoint
        if from_epoch_and_ids:
            max_epoch = from_epoch_and_ids[0]
            max_message_ids = from_epoch_and_ids[1]
        else:
            max_epoch = None
            max_message_ids = []

        for row in rows:
            #logging.debug(row)
            total_count += 1

            processed_count += 1
            row['_time'] = row['pulled']

            row['processed'] = datetime.utcnow().replace(tzinfo=UTC).astimezone(self.tz)

            row['epoch_received'] = unix_time_epoch(row, 'received')
            item_epoch = unix_time_epoch(row, 'pulled')
            if item_epoch > max_epoch:
                max_epoch = item_epoch
                max_message_ids = [ row['message_id'] ]
            else:
                max_message_ids.append(row['message_id'])

            # remove all null fields because Splunk will show them as 'null'
            row_no_none = {k: v for k, v in row.items() if v is not None}
            print json.dumps(row_no_none, default=default, sort_keys=True)
            #logging.warning(json.dumps(row, default=default, sort_keys=True))

        logging.warning('Processed %d out of %d messages' % (processed_count, total_count))
        if max_epoch:
            return (max_epoch, max_message_ids)
        else:
            return None
        

def run():
	config = get_config()

        name = config["name"]
	mailbox = config["mailbox"]
	domain = config["domain"]
	username = config["username"]
	password = config["password"]
	folder = config.get("folder")
        if "max_count" in config:
            max_count = int(config["max_count"])
        else:
            max_count = 100

        if "lag_seconds" in config:
            lag_seconds = int(config["lag_seconds"])
        else:
            lag_seconds = 3600

        session_key = config["session_key"]
        
        if session_key:

            try:
                if password != MASK:
                    logging.info("Encrypting password for username=%s" % username)
                    encrypt_password(username, password, session_key)
                    mask_password(name, session_key, username)
                clear_password = get_password(session_key, username)
                logging.info("Using user %s" % username)

                from_epoch_and_ids = load_checkpoint(config)

                poller = Poller()
                max_epoch_and_ids = poller.poll(domain, username, mailbox, clear_password,
                    folder,
                    max_count,
                    lag_seconds,
                    from_epoch_and_ids
                    )
                if max_epoch_and_ids:
                    save_checkpoint(config, max_epoch_and_ids[0], max_epoch_and_ids[1])

            except Exception as e:
                logging.exception("Error while polling: %s" % str(e))


if __name__ == '__main__':
    if len(sys.argv) > 1 and sys.argv[1] == 'conda':
        argv = sys.argv[1:]
        if len(argv) > 1:
            if argv[1] == "--scheme":
                do_scheme()
            elif argv[1] == "--validate-arguments":
                if len(argv) > 3:
                    validate_config(argv[2], argv[3])
                else:
                    print 'supply username and password'
            elif argv[1] == "--test":
                print 'No tests for the scheme present'
            else:
                print 'You giveth weird arguments'
        else:
            # just request data from Twitter
            run()

        sys.exit(0)
    else:
        print "script should be called from conda and passing conda as the first argument"
        sys.exit(1)

